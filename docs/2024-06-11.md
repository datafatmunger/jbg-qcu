## 2024-06-11 QIS/QCS Simulator in Nvidia CUDA, p.2 (en_US)

Working on a quantum universal gate simulator for educational purposes, non-production ready, designed to run on Nvidia hardware via CUDA.

Source code will be developing on [Github](https://github.com/datafatmunger/jbg-qcu),
with some of the process documented on this blog.

Said "Q Coo", like "cuckoo" (will create a cute bird logo later). Is a POC, non-production, quantum universal gate simulator designed to run on Nvidia hardward via CUDA. Mainly to assist in educating myself on quantum computer simulation.

### Why Nvidia?

I saw Nvidia surpassed Apple in market cap?! Maybe not a bad time to do linear algebra on Nvidia hardware.

I also really have a ♥ affair with my GPU (RTX 3070), which I aquired during the pandemic & supply chain crisis, from a scalper, to whom I drove 2 hours, & paid way too much money (€1500).

### Matrix / vector multiplication (y = A * x)

Made some progress yesterday with the following.

```

#include <iostream>
#include <cuda_runtime.h>

#define BLOCK_SIZE 16

// CUDA kernel for matrix-vector multiplication
__global__ void matVecMulKernel(float *A, float *x, float *y, int rows, int cols) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    if (row < rows) {
        float sum = 0.0f;
        for (int col = 0; col < cols; ++col) {
            sum += A[row * cols + col] * x[col];
        }
        y[row] = sum;
    }
}

void matVecMul(float *h_A, float *h_x, float *h_y, int rows, int cols) {
    float *d_A, *d_x, *d_y;

    // Allocate memory on the device
    cudaMalloc((void**)&d_A, rows * cols * sizeof(float));
    cudaMalloc((void**)&d_x, cols * sizeof(float));
    cudaMalloc((void**)&d_y, rows * sizeof(float));

    // Copy data from host to device
    cudaMemcpy(d_A, h_A, rows * cols * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, h_x, cols * sizeof(float), cudaMemcpyHostToDevice);

    // Define the grid and block dimensions
    dim3 dimBlock(1, BLOCK_SIZE);
    dim3 dimGrid(1, (rows + BLOCK_SIZE - 1) / BLOCK_SIZE);

    // Launch the kernel
    matVecMulKernel<<<dimGrid, dimBlock>>>(d_A, d_x, d_y, rows, cols);

    // Copy the result back to the host
    cudaMemcpy(h_y, d_y, rows * sizeof(float), cudaMemcpyDeviceToHost);

    // Clean up
    cudaFree(d_A);
    cudaFree(d_x);
    cudaFree(d_y);
}

int main() {
    int rows = 4;
    int cols = 4;

    // Host input matrices
    float h_A[] = {
        1,  0,  1,  0,
        0,  1,  0,  1,
        1,  0, -1,  0,
        0,  1,  0, -1};
    float h_x[] = {1, 0, 0, 0};
    float h_y[4];

    // Matrix-vector multiplication
    matVecMul(h_A, h_x, h_y, rows, cols);

    // Print the result
    for (int i = 0; i < rows; ++i) {
        std::cout << h_y[i] << " ";
    }
    std::cout << std::endl;

    return 0;
}


```

### Tensor Product (A ⊗ B)

```

#include <cuda_runtime.h>
#include <iostream>

__global__ void tensorProductKernel(float* d_A, float* d_B, float* d_C, int aRows, int aCols, int bRows, int bCols) {
    int i = blockIdx.y * blockDim.y + threadIdx.y;
    int j = blockIdx.x * blockDim.x + threadIdx.x;
    int totalRows = aRows * bRows;
    int totalCols = aCols * bCols;

    if (i < totalRows && j < totalCols) {
        int rowA = i / bRows;
        int colA = j / bCols;
        int rowB = i % bRows;
        int colB = j % bCols;
        d_C[i * totalCols + j] = d_A[rowA * aCols + colA] * d_B[rowB * bCols + colB];
    }
}

void tensorProduct(float* h_A, float* h_B, float* h_C, int aRows, int aCols, int bRows, int bCols) {
    int aSize = aRows * aCols * sizeof(float);
    int bSize = bRows * bCols * sizeof(float);
    int cSize = aRows * bRows * aCols * bCols * sizeof(float);

    float* d_A;
    float* d_B;
    float* d_C;

    cudaMalloc((void**)&d_A, aSize);
    cudaMalloc((void**)&d_B, bSize);
    cudaMalloc((void**)&d_C, cSize);

    cudaMemcpy(d_A, h_A, aSize, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, bSize, cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks((aCols * bCols + threadsPerBlock.x - 1) / threadsPerBlock.x, 
                   (aRows * bRows + threadsPerBlock.y - 1) / threadsPerBlock.y);

    tensorProductKernel<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, aRows, aCols, bRows, bCols);

    cudaMemcpy(h_C, d_C, cSize, cudaMemcpyDeviceToHost);

    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
}

int main() {
    const int aRows = 2;
    const int aCols = 2;
    const int bRows = 2;
    const int bCols = 2;

    float h_A[aRows * aCols] = {1, 1, 1, -1};
    float h_B[bRows * bCols] = {1, 0, 0, 1};
    float h_C[aRows * bRows * aCols * bCols];

    tensorProduct(h_A, h_B, h_C, aRows, aCols, bRows, bCols);

    for (int i = 0; i < aRows * bRows; ++i) {
        for (int j = 0; j < aCols * bCols; ++j) {
            std::cout << h_C[i * aCols * bCols + j] << " ";
        }
        std::cout << "\n";
    }

    return 0;
}


```

### Checkpoint notes

Neither of these are handling complex numbers, and I would like to pull them into more of a stdlib with a header, and change the prints to "tests".

## Source control

Creating a Github repository for Qcu.

### Creating a ed25519 ssh key on Windows

        ssh-keygen -t ed25519

### Reminder: Windows version of `cat` is `type`

        type .ssh\id_ed25519.pub

## Creating a markdown → html pipeline

Ideally I would be able to convert these notes to HTML for my blog.



